{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This file provides starter code for extracting features from the xml files and\n",
    "## for doing some learning.\n",
    "##\n",
    "## The basic set-up: \n",
    "## ----------------\n",
    "## main() will run code to extract features, learn, and make predictions.\n",
    "## \n",
    "## extract_feats() is called by main(), and it will iterate through the \n",
    "## train/test directories and parse each xml file into an xml.etree.ElementTree, \n",
    "## which is a standard python object used to represent an xml file in memory.\n",
    "## (More information about xml.etree.ElementTree objects can be found here:\n",
    "## http://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "## and here: http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/)\n",
    "## It will then use a series of \"feature-functions\" that you will write/modify\n",
    "## in order to extract dictionaries of features from each ElementTree object.\n",
    "## Finally, it will produce an N x D sparse design matrix containing the union\n",
    "## of the features contained in the dictionaries produced by your \"feature-functions.\"\n",
    "## This matrix can then be plugged into your learning algorithm.\n",
    "##\n",
    "## The learning and prediction parts of main() are largely left to you, though\n",
    "## it does contain code that randomly picks class-specific weights and predicts\n",
    "## the class with the weights that give the highest score. If your prediction\n",
    "## algorithm involves class-specific weights, you should, of course, learn \n",
    "## these class-specific weights in a more intelligent way.\n",
    "##\n",
    "## Feature-functions:\n",
    "## --------------------\n",
    "## \"feature-functions\" are functions that take an ElementTree object representing\n",
    "## an xml file (which contains, among other things, the sequence of system calls a\n",
    "## piece of potential malware has made), and returns a dictionary mapping feature names to \n",
    "## their respective numeric values. \n",
    "## For instance, a simple feature-function might map a system call history to the\n",
    "## dictionary {'first_call-load_image': 1}. This is a boolean feature indicating\n",
    "## whether the first system call made by the executable was 'load_image'. \n",
    "## Real-valued or count-based features can of course also be defined in this way. \n",
    "## Because this feature-function will be run over ElementTree objects for each \n",
    "## software execution history instance, we will have the (different)\n",
    "## feature values of this feature for each history, and these values will make up \n",
    "## one of the columns in our final design matrix.\n",
    "## Of course, multiple features can be defined within a single dictionary, and in\n",
    "## the end all the dictionaries returned by feature functions (for a particular\n",
    "## training example) will be unioned, so we can collect all the feature values \n",
    "## associated with that particular instance.\n",
    "##\n",
    "## Two example feature-functions, first_last_system_call_feats() and \n",
    "## system_call_count_feats(), are defined below.\n",
    "## The first of these functions indicates what the first and last system-calls \n",
    "## made by an executable are, and the second records the total number of system\n",
    "## calls made by an executable.\n",
    "##\n",
    "## What you need to do:\n",
    "## --------------------\n",
    "## 1. Write new feature-functions (or modify the example feature-functions) to\n",
    "## extract useful features for this prediction task.\n",
    "## 2. Implement an algorithm to learn from the design matrix produced, and to\n",
    "## make predictions on unseen data. Naive code for these two steps is provided\n",
    "## below, and marked by TODOs.\n",
    "##\n",
    "## Computational Caveat\n",
    "## --------------------\n",
    "## Because the biggest of any of the xml files is only around 35MB, the code below \n",
    "## will parse an entire xml file and store it in memory, compute features, and\n",
    "## then get rid of it before parsing the next one. Storing the biggest of the files \n",
    "## in memory should require at most 200MB or so, which should be no problem for\n",
    "## reasonably modern laptops. If this is too much, however, you can lower the\n",
    "## memory requirement by using ElementTree.iterparse(), which does parsing in\n",
    "## a streaming way. See http://eli.thegreenplace.net/2012/03/15/processing-xml-in-python-with-elementtree/\n",
    "## for an example. \n",
    "\n",
    "import os\n",
    "from collections import Counter\n",
    "try:\n",
    "    import xml.etree.cElementTree as ET\n",
    "except ImportError:\n",
    "    import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "import util\n",
    "\n",
    "\n",
    "# In[93]:\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_feats(ffs, direc=\"train\", global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "    returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    fds = [] # list of feature dicts\n",
    "    classes = []\n",
    "    ids = []\n",
    "    counting = 0\n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        tree = ET.parse(os.path.join(direc,datafile))    \n",
    "                \n",
    "        # accumulate features\n",
    "        [rowfd.update(ff(tree)) for ff in ffs]\n",
    "        fds.append(rowfd)\n",
    "           \n",
    "    X,feat_dict = make_design_mat(fds,global_feat_dict)\n",
    "    return X, feat_dict, np.array(classes), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_design_mat(fds, global_feat_dict=None):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      fds is a list of feature dicts (one for each row).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "       \n",
    "    returns: \n",
    "        a sparse NxD design matrix, where N == len(fds) and D is the number of\n",
    "        the union of features defined in any of the fds \n",
    "    \"\"\"\n",
    "    if global_feat_dict is None:\n",
    "        all_feats = set()\n",
    "        [all_feats.update(fd.keys()) for fd in fds]\n",
    "        feat_dict = dict([(feat, i) for i, feat in enumerate(sorted(all_feats))])\n",
    "    else:\n",
    "        feat_dict = global_feat_dict\n",
    "        \n",
    "    cols = []\n",
    "    rows = []\n",
    "    data = []        \n",
    "    for i in range(len(fds)):\n",
    "        temp_cols = []\n",
    "        temp_data = []\n",
    "        for feat,val in fds[i].items():\n",
    "            try:\n",
    "                # update temp_cols iff update temp_data\n",
    "                temp_cols.append(feat_dict[feat])\n",
    "                temp_data.append(val)\n",
    "            except KeyError as ex:\n",
    "                if global_feat_dict is not None:\n",
    "                    pass  # new feature in test data; nbd\n",
    "                else:\n",
    "                    raise ex\n",
    "\n",
    "        # all fd's features in the same row\n",
    "        k = len(temp_cols)\n",
    "        cols.extend(temp_cols)\n",
    "        data.extend(temp_data)\n",
    "        rows.extend([i]*k)\n",
    "\n",
    "    assert len(cols) == len(rows) and len(rows) == len(data)\n",
    "   \n",
    "    \n",
    "    X = sparse.csr_matrix((np.array(data),\n",
    "                   (np.array(rows), np.array(cols))),\n",
    "                   shape=(len(fds), len(feat_dict)))\n",
    "    return X, feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Here are two example feature-functions. They each take an xml.etree.ElementTree object, \n",
    "# (i.e., the result of parsing an xml file) and returns a dictionary mapping \n",
    "# feature-names to numeric values.\n",
    "## TODO: modify these functions, and/or add new ones.\n",
    "def first_last_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'first_call-x' to 1 if x was the first system call\n",
    "      made, and 'last_call-y' to 1 if y was the last system call made. \n",
    "      (in other words, it returns a dictionary indicating what the first and \n",
    "      last system calls made by an executable were.)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    first = True # is this the first system call\n",
    "    last_call = None # keep track of last call we've seen\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            if first:\n",
    "                c[\"first_call-\"+el.tag] = 1\n",
    "                first = False\n",
    "            last_call = el.tag  # update last call seen\n",
    "            \n",
    "    # finally, mark last call seen\n",
    "    c[\"last_call-\"+last_call] = 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_count_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c['num_system_calls'] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_histogram_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c['hist-'+el.tag] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_frequent_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag] += 1\n",
    "    d = Counter()\n",
    "    key, val = c.most_common(1)[0]\n",
    "    #print(key, val)\n",
    "    d[key] = val\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distinct_system_call_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag] += 1\n",
    "\n",
    "    d = Counter()\n",
    "    d['num_distinct_calls'] = len(c)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_histogram_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c['hist-'+el.tag] += 1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_histogram_feats_normalized(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = system_call_histogram_feats(tree)\n",
    "\n",
    "    #normalize the counter https://stackoverflow.com/questions/22428842/how-to-normalize-a-counter-and-combine-2-normalized-counters-python\n",
    "    total = sum(c.values(), 0.0)\n",
    "    for key in c:\n",
    "        c[key] /= total\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_bigram_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag] += 1\n",
    "\n",
    "    d = Counter()\n",
    "    # iterate over all pairs\n",
    "    for k1 in list(c):\n",
    "        for k2 in list(c):\n",
    "            combo_key = '-'.join(sorted([k1, k2]))\n",
    "            combo_key = 'bigram-'+combo_key\n",
    "            #print(combo_key)\n",
    "            d[combo_key] = 1\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_ordered_bigram_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    \n",
    "    last1 = ''\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag+'-'+last1] += 1\n",
    "        last1 = el.tag\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_ordered_bigram_feats_normalized(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = system_call_ordered_bigram_feats(tree)\n",
    "\n",
    "    #normalize the counter https://stackoverflow.com/questions/22428842/how-to-normalize-a-counter-and-combine-2-normalized-counters-python\n",
    "    total = sum(c.values(), 0.0)\n",
    "    for key in c:\n",
    "        c[key] /= total\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_ordered_trigram_feats(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    \n",
    "    last1 = ''\n",
    "    last2 = ''\n",
    "    c = Counter()\n",
    "    in_all_section = False\n",
    "    for el in tree.iter():\n",
    "        # ignore everything outside the \"all_section\" element\n",
    "        if el.tag == \"all_section\" and not in_all_section:\n",
    "            in_all_section = True\n",
    "        elif el.tag == \"all_section\" and in_all_section:\n",
    "            in_all_section = False\n",
    "        elif in_all_section:\n",
    "            c[el.tag+'-'+last1+'-'+last2] += 1\n",
    "        last2 = last1\n",
    "        last1 = el.tag\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def system_call_ordered_trigram_feats_normalized(tree):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      tree is an xml.etree.ElementTree object\n",
    "    returns:\n",
    "      a dictionary mapping 'num_system_calls' to the number of system_calls\n",
    "      made by an executable (summed over all processes)\n",
    "    \"\"\"\n",
    "    c = system_call_ordered_trigram_feats(tree)\n",
    "\n",
    "    #normalize the counter https://stackoverflow.com/questions/22428842/how-to-normalize-a-counter-and-combine-2-normalized-counters-python\n",
    "    total = sum(c.values(), 0.0)\n",
    "    for key in c:\n",
    "        c[key] /= total\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract__better_feats(vectorizer, scaler, direc=\"train\", training=True):\n",
    "    \"\"\"\n",
    "    arguments:\n",
    "      ffs are a list of feature-functions.\n",
    "      direc is a directory containing xml files (expected to be train or test).\n",
    "      global_feat_dict is a dictionary mapping feature_names to column-numbers; it\n",
    "      should only be provided when extracting features from test data, so that \n",
    "      the columns of the test matrix align correctly.\n",
    "\n",
    "  \n",
    "  returns: \n",
    "      a sparse design matrix, a dict mapping features to column-numbers,\n",
    "      a vector of target classes, and a list of system-call-history ids in order \n",
    "      of their rows in the design matrix.\n",
    "      \n",
    "      Note: the vector of target classes returned will contain the true indices of the\n",
    "      target classes on the training data, but will contain only -1's on the test\n",
    "      data\n",
    "    \"\"\"\n",
    "    classes = []\n",
    "    ids = []\n",
    "    cv_files = []\n",
    "    counting = 0\n",
    "    for datafile in os.listdir(direc):\n",
    "        # extract id and true class (if available) from filename\n",
    "        id_str,clazz = datafile.split('.')[:2]\n",
    "        ids.append(id_str)\n",
    "        # add target class if this is training data\n",
    "        try:\n",
    "            classes.append(util.malware_classes.index(clazz))\n",
    "        except ValueError:\n",
    "            # we should only fail to find the label in our list of malware classes\n",
    "            # if this is test data, which always has an \"X\" label\n",
    "            assert clazz == \"X\"\n",
    "            classes.append(-1)\n",
    "        rowfd = {}\n",
    "        # parse file as an xml document\n",
    "        #tree = ET.parse(os.path.join(direc,datafile))\n",
    "        cv_files.append(os.path.join(direc,datafile))  \n",
    "\n",
    "    if training == False:\n",
    "        raw_X = vectorizer.transform(cv_files)\n",
    "        X = scaler.transform(raw_X)\n",
    "    elif training == True:\n",
    "        raw_X = vectorizer.fit_transform(cv_files)\n",
    "        X = scaler.fit_transform(raw_X)\n",
    "\n",
    "    return X, vectorizer, scaler, np.array(classes), ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting training features...\n",
      "(3086, 17116)\n",
      "[ 8 12 10 ..., 13 10 10]\n",
      "done extracting training features\n",
      "\n",
      "(2625, 17116)\n",
      "(461, 17116)\n",
      "(2625,)\n",
      "(461,)\n"
     ]
    }
   ],
   "source": [
    "    train_dir = \"train\"\n",
    "    test_dir = \"test\"\n",
    "    outputfile = \"sample_predictions.csv\"  # feel free to change this or take it as an argument\n",
    "\n",
    "    # TODO put the names of the feature functions you've defined above in this list\n",
    "    #ffs = [first_last_system_call_feats, system_call_count_feats, system_call_histogram_feats, distinct_system_call_feats, most_frequent_system_call_feats, system_call_bigram_feats]\n",
    "    #ffs = [system_call_ordered_bigram_feats_normalized,system_call_ordered_bigram_feats,system_call_ordered_trigram_feats_normalized,system_call_ordered_trigram_feats,system_call_count_feats]\n",
    "    ffs = [first_last_system_call_feats, system_call_count_feats, system_call_histogram_feats, distinct_system_call_feats, most_frequent_system_call_feats, system_call_ordered_bigram_feats_normalized,system_call_ordered_bigram_feats,system_call_ordered_trigram_feats_normalized,system_call_ordered_trigram_feats,system_call_bigram_feats]\n",
    "\n",
    "    # extract features\n",
    "    print \"extracting training features...\"\n",
    "    X_train,global_feat_dict,t_train,train_ids = extract_feats(ffs, train_dir)\n",
    "    print(X_train.shape)\n",
    "    print(t_train)\n",
    "    print \"done extracting training features\"\n",
    "    print\n",
    "\n",
    "    #turn sparse matrix back into an array and separate into training and validation data\n",
    "    X_mat = X_train.toarray()\n",
    "    X_tr = X_mat[0:2625,:]\n",
    "    X_val = X_mat[2625:3086,:]\n",
    "    t_tr = t_train[0:2625]\n",
    "    t_val = t_train[2625:3086]\n",
    "    print(X_tr.shape)\n",
    "    print(X_val.shape)\n",
    "    print(t_tr.shape)\n",
    "    print(t_val.shape)\n",
    "\n",
    "    # TODO train here, and learn your classification parameters\n",
    "    #print \"learning...\"\n",
    "    #learned_W = np.random.random((len(global_feat_dict),len(util.malware_classes)))\n",
    "    #print \"done learning\"\n",
    "    #print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting test features...\n",
      "done extracting test features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"extracting test features...\")\n",
    "X_test,_,t_ignore,test_ids = extract_feats(ffs, test_dir, global_feat_dict=global_feat_dict)\n",
    "print(\"done extracting test features\")\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVD = TruncatedSVD(n_components=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2625, 1000)\n",
      "(461, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_tr_SVD = SVD.fit_transform(X_tr)\n",
    "X_val_SVD = SVD.transform(X_val)\n",
    "print X_tr_SVD.shape\n",
    "print X_val_SVD.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of MLP model given validation data:', 0.8850325379609545)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "#MLP Classification \n",
    "mlp_model_SVD= MLPClassifier(alpha=1e-03, hidden_layer_sizes=(1000,))\n",
    "mlp_model_SVD.fit(X_tr_SVD, t_tr)\n",
    "mlp_preds_SVD = mlp_model_SVD.predict(X_val_SVD)\n",
    "B = t_val == mlp_preds_SVD \n",
    "\n",
    "acc = 1.*sum(B)/len(B)\n",
    "print(\"Accuracy of MLP model given validation data:\", acc)\n",
    "print mlp_model_SVD.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #CV = CountVectorizer(input='filename', encoding='utf-8',\n",
    "                         #decode_error='strict', strip_accents='ascii',\n",
    "                         #lowercase=True, preprocessor=None,\n",
    "                         #tokenizer=None, stop_words=None,\n",
    "                         #token_pattern=’(?u)\\b\\w\\w+\\b’,\n",
    "                         #ngram_range=(1, 5),\n",
    "                         #analyzer='word', max_df=1.0, min_df=1,\n",
    "                         #max_features=None, vocabulary=None\n",
    "                         #binary=False,\n",
    "                         #dtype=<class 'numpy.int64'>\n",
    "                         #dtype=np.uint8\n",
    "                        #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prep variable call\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf-8',\n",
    "                         decode_error='strict', strip_accents='ascii',\n",
    "                         lowercase=True, preprocessor=None,\n",
    "                         tokenizer=None, stop_words=None,\n",
    "                         #token_pattern=’(?u)\\b\\w\\w+\\b’,\n",
    "                         ngram_range=(1, 2),\n",
    "                         analyzer='word', max_df=1.0, min_df=1,\n",
    "                         max_features=None, vocabulary=None,\n",
    "                         #binary=False,\n",
    "                         #dtype=<class 'numpy.int64'>\n",
    "                         dtype=np.uint8#\n",
    "                        )\n",
    "scaler = StandardScaler(copy=False, with_mean=False, with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, vectorizer, svd, Y, ids = extract__better_feats(vectorizer, scaler, direc=\"train\", training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xt, vectorizer, svd, Yt, idst = extract__better_feats(vectorizer, scaler, direc=\"test\", training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print type(Xt)\n",
    "print type(Xt.shape)\n",
    "print np.ndarray.max(Xt.toarray())\n",
    "print np.ndarray.min(Xt.toarray())\n",
    "print type(X)\n",
    "print type(X.shape)\n",
    "print np.ndarray.max(X.toarray())\n",
    "print np.ndarray.min(X.toarray())\n",
    "print type(X_mat)\n",
    "print type(X_mat.shape)\n",
    "print np.ndarray.max(X_mat.toarray())\n",
    "print np.ndarray.min(X_mat.toarray())\n",
    "print type(X_test)\n",
    "print type(X_test.shape)\n",
    "print np.ndarray.max(X_test.toarray())\n",
    "print np.ndarray.min(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print vectorizer.vocabulary_\n",
    "print vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2625, 1386)\n",
      "(461, 1386)\n",
      "(2625,)\n",
      "(461,)\n"
     ]
    }
   ],
   "source": [
    "    #turn sparse matrix back into an array and separate into training and validation data\n",
    "    X_mat1 = X.toarray()\n",
    "    X_test1 = Xt.toarray()\n",
    "    X_tr1 = X_mat1[0:2625,:]\n",
    "    X_val1 = X_mat1[2625:3086,:]\n",
    "    t_tr1 = Y[0:2625]\n",
    "    t_val1 = Y[2625:3086]\n",
    "    print(X_tr1.shape)\n",
    "    print(X_val1.shape)\n",
    "    print(t_tr1.shape)\n",
    "    print(t_val1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print X_tr.shape\n",
    "print X_tr1.toarray().shape\n",
    "\n",
    "print type(X_tr)\n",
    "print type(X_tr1.toarray())\n",
    "print type(X_test)\n",
    "print type(Xt.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr2 = np.concatenate((X_tr, X_tr1.toarray()), axis=1)\n",
    "X_val2 = np.concatenate((X_val, X_val1.toarray()), axis=1)\n",
    "print X_tr2.shape\n",
    "print X_val2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of MLP model given validation data:', 0.8915401301518439)\n"
     ]
    }
   ],
   "source": [
    "#MLP Classification \n",
    "mlp_model= MLPClassifier(alpha=1e-05)\n",
    "mlp_model.fit(X_tr2, t_tr1)\n",
    "mlp_preds = mlp_model.predict(X_val2)\n",
    "B = t_val1 == mlp_preds \n",
    "\n",
    "acc = 1.*sum(B)/len(B)\n",
    "print(\"Accuracy of MLP model given validation data:\", acc)\n",
    "print mlp_model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr3 = SVD.fit_transform(X_tr2)\n",
    "X_val3 = SVD.transform(X_val2)\n",
    "print X_tr3.shape\n",
    "print X_val3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of MLP model given validation data:', 0.8980477223427332)\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#MLP Classification \n",
    "mlp_model= MLPClassifier(alpha=1e-05)\n",
    "mlp_model.fit(X_tr3, t_tr)\n",
    "mlp_preds = mlp_model.predict(X_val3)\n",
    "B = t_val1 == mlp_preds \n",
    "\n",
    "acc = 1.*sum(B)/len(B)\n",
    "print(\"Accuracy of MLP model given validation data:\", acc)\n",
    "print mlp_model.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alpha 0.0001', 'alpha 1e-05', 'alpha 1e-06', 'alpha 1e-07', 'alpha 1e-08']\n"
     ]
    }
   ],
   "source": [
    "alphas = 10.0 ** -np.arange(4, 9)\n",
    "names = []\n",
    "for i in alphas:\n",
    "    names.append('alpha ' + str(i))\n",
    "print names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[200], [400], [600]]\n"
     ]
    }
   ],
   "source": [
    "SIZES = [[200,], [400,], [600,]] \n",
    "print SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    #('reduce_dim', PCA()),\n",
    "    ('classify', MLPClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'classify__hidden_layer_sizes': [[200], [400], [600]], 'classify__alpha': array([  1.00000000e-04,   1.00000000e-05,   1.00000000e-06,\n",
      "         1.00000000e-07,   1.00000000e-08])}]\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "#        'reduce_dim': [PCA(iterated_power=7), NMF()],\n",
    "#        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n",
    "        'classify__hidden_layer_sizes': SIZES,\n",
    "        'classify__alpha': alphas\n",
    "    }\n",
    "]\n",
    "print param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, error_score='raise',\n",
      "       estimator=Pipeline(memory=None,\n",
      "     steps=[('classify', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))]),\n",
      "       fit_params=None, iid=True, n_jobs=1,\n",
      "       param_grid=[{'classify__hidden_layer_sizes': [[200], [400], [600]], 'classify__alpha': array([  1.00000e-04,   1.00000e-05,   1.00000e-06,   1.00000e-07,\n",
      "         1.00000e-08])}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=100)\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid,verbose=100)\n",
    "print grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.1 .........\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.1, score=0.846590909091, total=   2.1s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.1s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.1 .........\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.1, score=0.867579908676, total=   1.9s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.0s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.1 .........\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.1, score=0.853855005754, total=   1.8s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.9s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.1 .........\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.1, score=0.856818181818, total=   6.9s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   12.9s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.1 .........\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.1, score=0.868721461187, total=   4.4s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   17.4s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.1 .........\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.1, score=0.861910241657, total=   7.4s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   24.9s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.1 ........\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.1, score=0.845454545455, total=  13.7s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   38.7s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.1 ........\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.1, score=0.87100456621, total=  11.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   49.9s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.1 ........\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.1, score=0.859608745685, total=   9.9s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   59.9s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.01 ........\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.01, score=0.845454545455, total=   2.2s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.01 ........\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.01, score=0.880136986301, total=   2.7s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.01 ........\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.01, score=0.851553509781, total=   2.3s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.01 ........\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.01, score=0.864772727273, total=  11.6s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.01 ........\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.01, score=0.850456621005, total=   6.8s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.01 ........\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.01, score=0.85500575374, total=   7.1s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.01 .......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.01, score=0.864772727273, total=  13.2s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  1.8min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.01 .......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.01, score=0.867579908676, total=  20.4s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.01 .......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.01, score=0.846950517837, total=   9.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.001 .......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.001, score=0.839772727273, total=   2.1s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.001 .......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.001, score=0.866438356164, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  2.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.001 .......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.001, score=0.851553509781, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  2.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.001 .......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.001, score=0.846590909091, total=   7.3s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:  2.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.001 .......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.001, score=0.866438356164, total=   4.8s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.001 .......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.001, score=0.874568469505, total=   5.5s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  2.7min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.001, score=0.848863636364, total=  11.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.001, score=0.869863013699, total=   8.8s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.001, score=0.860759493671, total=  12.1s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.0001, score=0.85, total=   2.4s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:  3.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.0001, score=0.877853881279, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  3.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=0.0001, score=0.859608745685, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.0001, score=0.8625, total=   5.5s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  3.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.0001, score=0.874429223744, total=   6.0s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=0.0001, score=0.848101265823, total=   6.3s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:  3.7min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.0001 .....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.0001, score=0.870454545455, total=  13.7s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.0001 .....\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.0001, score=0.87100456621, total=  14.5s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  4.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=0.0001 .....\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=0.0001, score=0.85500575374, total=  13.8s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  4.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=1e-05, score=0.845454545455, total=   2.1s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:  4.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=1e-05, score=0.873287671233, total=   2.3s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:  4.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[100], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[100], classify__alpha=1e-05, score=0.857307249712, total=   2.2s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:  4.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=1e-05, score=0.856818181818, total=   9.8s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  4.7min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=1e-05, score=0.877853881279, total=   6.4s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:  4.8min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[500], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[500], classify__alpha=1e-05, score=0.866513233602, total=   9.0s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:  4.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=1e-05 ......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=1e-05, score=0.856818181818, total=  13.6s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:  5.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=1e-05 ......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=1e-05, score=0.872146118721, total=  10.4s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:  5.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[1000], classify__alpha=1e-05 ......\n",
      "[CV]  classify__hidden_layer_sizes=[1000], classify__alpha=1e-05, score=0.858457997699, total=   7.6s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  5.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  5.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('classify', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'classify__hidden_layer_sizes': [[100], [500], [1000]], 'classify__alpha': array([  1.00000e-01,   1.00000e-02,   1.00000e-03,   1.00000e-04,\n",
       "         1.00000e-05])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=100)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_tr_SVD, t_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=0.0001, score=0.8625, total=   4.6s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.6s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=0.0001, score=0.876712328767, total=   3.5s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    8.2s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=0.0001, score=0.851553509781, total=   3.4s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   11.6s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=0.0001, score=0.851136363636, total=   4.3s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   15.9s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=0.0001, score=0.873287671233, total=   5.4s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   21.4s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=0.0001, score=0.85500575374, total=   4.3s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   25.8s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=0.0001, score=0.868181818182, total=  10.8s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   36.7s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=0.0001, score=0.873287671233, total=   6.3s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   43.1s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=0.0001 ......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=0.0001, score=0.867663981588, total=  11.8s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   55.1s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-05, score=0.855681818182, total=   3.3s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   58.4s remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-05, score=0.866438356164, total=   3.5s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-05, score=0.848101265823, total=   3.4s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-05, score=0.857954545455, total=   6.5s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-05, score=0.872146118721, total=   6.6s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-05, score=0.858457997699, total=   5.2s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-05, score=0.857954545455, total=   8.6s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-05, score=0.87899543379, total=  11.3s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-05 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-05, score=0.861910241657, total=   7.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  1.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-06, score=0.852272727273, total=   3.2s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  1.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-06, score=0.868721461187, total=   2.8s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-06, score=0.859608745685, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-06, score=0.855681818182, total=   5.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-06, score=0.875570776256, total=   7.2s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:  2.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-06, score=0.848101265823, total=   5.4s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-06, score=0.854545454545, total=   6.6s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  2.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-06, score=0.85502283105, total=   5.1s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  2.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-06 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-06, score=0.866513233602, total=   8.7s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-07, score=0.844318181818, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:  2.7min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-07, score=0.874429223744, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  2.8min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-07, score=0.849252013809, total=   4.0s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  2.8min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-07, score=0.852272727273, total=   3.6s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-07, score=0.874429223744, total=   8.1s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-07 .......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-07, score=0.858457997699, total=   5.0s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:  3.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-07, score=0.846590909091, total=   7.2s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-07, score=0.852739726027, total=   8.0s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-07 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-07, score=0.850402761795, total=   7.1s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  3.5min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-08, score=0.859090909091, total=   4.8s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-08, score=0.866438356164, total=   3.5s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[200], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[200], classify__alpha=1e-08, score=0.849252013809, total=   4.2s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:  3.7min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-08, score=0.859090909091, total=   8.5s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  3.8min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-08, score=0.872146118721, total=   7.0s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[400], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[400], classify__alpha=1e-08, score=0.86996547756, total=   6.6s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:  4.1min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-08, score=0.863636363636, total=   8.3s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:  4.2min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-08, score=0.860730593607, total=   7.5s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:  4.3min remaining:    0.0s\n",
      "[CV] classify__hidden_layer_sizes=[600], classify__alpha=1e-08 .......\n",
      "[CV]  classify__hidden_layer_sizes=[600], classify__alpha=1e-08, score=0.857307249712, total=   5.0s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  4.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('classify', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'classify__hidden_layer_sizes': [[200], [400], [600]], 'classify__alpha': array([  1.00000e-04,   1.00000e-05,   1.00000e-06,   1.00000e-07,\n",
       "         1.00000e-08])}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=100)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_tr_SVD, t_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of MLP model given validation data:', 0.8872017353579176)\n"
     ]
    }
   ],
   "source": [
    "mlp_grid_preds = grid.predict(X_val_SVD)\n",
    "B = t_val == mlp_grid_preds\n",
    "\n",
    "acc = 1.*sum(B)/len(B)\n",
    "print(\"Accuracy of MLP model given validation data:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator__classify__tol': 0.0001, 'estimator__classify__verbose': False, 'n_jobs': 1, 'verbose': 100, 'estimator__classify__nesterovs_momentum': True, 'estimator__classify__warm_start': False, 'estimator__classify__max_iter': 200, 'estimator__memory': None, 'estimator__classify__random_state': None, 'estimator__steps': [('classify', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))], 'estimator__classify__momentum': 0.9, 'estimator__classify__learning_rate_init': 0.001, 'param_grid': [{'classify__hidden_layer_sizes': [[100], [500], [1000]], 'classify__alpha': array([  1.00000000e-01,   1.00000000e-02,   1.00000000e-03,\n",
      "         1.00000000e-04,   1.00000000e-05])}], 'cv': 3, 'scoring': None, 'estimator__classify__epsilon': 1e-08, 'estimator__classify__alpha': 0.0001, 'estimator__classify__solver': 'adam', 'estimator__classify__batch_size': 'auto', 'estimator__classify__learning_rate': 'constant', 'pre_dispatch': '2*n_jobs', 'estimator__classify': MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False), 'estimator__classify__activation': 'relu', 'estimator__classify__hidden_layer_sizes': (100,), 'fit_params': None, 'estimator__classify__beta_1': 0.9, 'estimator__classify__beta_2': 0.999, 'estimator__classify__shuffle': True, 'refit': True, 'estimator__classify__early_stopping': False, 'iid': True, 'estimator__classify__validation_fraction': 0.1, 'estimator__classify__power_t': 0.5, 'return_train_score': 'warn', 'estimator': Pipeline(memory=None,\n",
      "     steps=[('classify', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))]), 'error_score': 'raise'}\n",
      "13\n",
      "{'classify__hidden_layer_sizes': [500], 'classify__alpha': 1.0000000000000001e-05}\n",
      "{'classify__hidden_layer_sizes': [500], 'classify__alpha': 1.0000000000000001e-05}\n",
      "0.867047619048\n",
      "Pipeline(memory=None,\n",
      "     steps=[('classify', MLPClassifier(activation='relu', alpha=1.0000000000000001e-05,\n",
      "       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
      "       epsilon=1e-08, hidden_layer_sizes=[500], learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
      "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False))])\n",
      "{'std_train_score': array([ 0.00306456,  0.0038169 ,  0.00317833,  0.00797077,  0.01159044,\n",
      "        0.01325512,  0.00977906,  0.0034277 ,  0.00439119,  0.00185978,\n",
      "        0.00961824,  0.00610813,  0.000744  ,  0.00168434,  0.00906657]), 'rank_test_score': array([14,  3, 11, 10, 13,  8, 15,  3,  8,  3,  7,  2, 11,  1,  3], dtype=int32), 'split1_train_score': array([ 0.97598628,  0.96855346,  0.97026872,  0.98284734,  0.96683819,\n",
      "        0.98399085,  0.97884505,  0.97084048,  0.96683819,  0.97427101,\n",
      "        0.97598628,  0.98170383,  0.97998856,  0.97141224,  0.96626644]), 'split2_train_score': array([ 0.97209567,  0.97779043,  0.97095672,  0.97323462,  0.95785877,\n",
      "        0.95216401,  0.98006834,  0.96697039,  0.96013667,  0.976082  ,\n",
      "        0.95615034,  0.96753986,  0.97949886,  0.97551253,  0.95728929]), 'std_score_time': array([ 0.00107106,  0.00040177,  0.00088229,  0.00117775,  0.00036205,\n",
      "        0.03555279,  0.00177759,  0.01197927,  0.01154885,  0.00184738,\n",
      "        0.00445518,  0.00956422,  0.00016388,  0.00750811,  0.00997072]), 'param_classify__hidden_layer_sizes': masked_array(data = [list([100]) list([500]) list([1000]) list([100]) list([500]) list([1000])\n",
      " list([100]) list([500]) list([1000]) list([100]) list([500]) list([1000])\n",
      " list([100]) list([500]) list([1000])],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False],\n",
      "       fill_value = ?)\n",
      ", 'split2_test_score': array([ 0.85385501,  0.86191024,  0.85960875,  0.85155351,  0.85500575,\n",
      "        0.84695052,  0.85155351,  0.87456847,  0.86075949,  0.85960875,\n",
      "        0.84810127,  0.85500575,  0.85730725,  0.86651323,  0.858458  ]), 'param_classify__alpha': masked_array(data = [0.10000000000000001 0.10000000000000001 0.10000000000000001 0.01 0.01 0.01\n",
      " 0.001 0.001 0.001 0.0001 0.0001 0.0001 1.0000000000000001e-05\n",
      " 1.0000000000000001e-05 1.0000000000000001e-05],\n",
      "             mask = [False False False False False False False False False False False False\n",
      " False False False],\n",
      "       fill_value = ?)\n",
      ", 'mean_fit_time': array([  1.91248027,   6.23514334,  11.44888735,   2.39514931,\n",
      "         8.44192227,  14.10101525,   2.57647498,   5.83785582,\n",
      "        10.56128033,   2.61498237,   5.88507239,  13.91699203,\n",
      "         2.21358236,   8.36436629,  10.46166428]), 'split0_train_score': array([ 0.96848138,  0.97191977,  0.96389685,  0.96332378,  0.98567335,\n",
      "        0.97363897,  0.95873926,  0.97535817,  0.97077364,  0.97879656,\n",
      "        0.97707736,  0.97879656,  0.9782235 ,  0.9730659 ,  0.97936963]), 'mean_score_time': array([ 0.00924738,  0.03335444,  0.06437373,  0.00961868,  0.03335373,\n",
      "        0.10974296,  0.01037637,  0.04465564,  0.07954105,  0.01188962,\n",
      "        0.03756356,  0.07504956,  0.00865563,  0.03877997,  0.08260171]), 'std_test_score': array([ 0.008715  ,  0.00488406,  0.01046959,  0.01513136,  0.00598345,\n",
      "        0.00911852,  0.0109283 ,  0.01175757,  0.00861358,  0.01156798,\n",
      "        0.01074739,  0.00740259,  0.01142236,  0.00861069,  0.00687618]), 'mean_train_score': array([ 0.97218778,  0.97275455,  0.9683741 ,  0.97313525,  0.97012344,\n",
      "        0.96993128,  0.97255088,  0.97105634,  0.96591617,  0.97638319,\n",
      "        0.96973799,  0.97601342,  0.97923697,  0.97333022,  0.96764179]), 'split0_test_score': array([ 0.84659091,  0.85681818,  0.84545455,  0.84545455,  0.86477273,\n",
      "        0.86477273,  0.83977273,  0.84659091,  0.84886364,  0.85      ,\n",
      "        0.8625    ,  0.87045455,  0.84545455,  0.85681818,  0.85681818]), 'mean_test_score': array([ 0.856     ,  0.86247619,  0.85866667,  0.85904762,  0.8567619 ,\n",
      "        0.85980952,  0.85257143,  0.86247619,  0.85980952,  0.86247619,\n",
      "        0.86171429,  0.86552381,  0.85866667,  0.86704762,  0.86247619]), 'params': [{'classify__hidden_layer_sizes': [100], 'classify__alpha': 0.10000000000000001}, {'classify__hidden_layer_sizes': [500], 'classify__alpha': 0.10000000000000001}, {'classify__hidden_layer_sizes': [1000], 'classify__alpha': 0.10000000000000001}, {'classify__hidden_layer_sizes': [100], 'classify__alpha': 0.01}, {'classify__hidden_layer_sizes': [500], 'classify__alpha': 0.01}, {'classify__hidden_layer_sizes': [1000], 'classify__alpha': 0.01}, {'classify__hidden_layer_sizes': [100], 'classify__alpha': 0.001}, {'classify__hidden_layer_sizes': [500], 'classify__alpha': 0.001}, {'classify__hidden_layer_sizes': [1000], 'classify__alpha': 0.001}, {'classify__hidden_layer_sizes': [100], 'classify__alpha': 0.0001}, {'classify__hidden_layer_sizes': [500], 'classify__alpha': 0.0001}, {'classify__hidden_layer_sizes': [1000], 'classify__alpha': 0.0001}, {'classify__hidden_layer_sizes': [100], 'classify__alpha': 1.0000000000000001e-05}, {'classify__hidden_layer_sizes': [500], 'classify__alpha': 1.0000000000000001e-05}, {'classify__hidden_layer_sizes': [1000], 'classify__alpha': 1.0000000000000001e-05}], 'std_fit_time': array([ 0.10773982,  1.30366188,  1.57829967,  0.18827974,  2.18652757,\n",
      "        4.67965372,  0.4161708 ,  1.03362486,  1.38954686,  0.18265153,\n",
      "        0.33263196,  0.37724331,  0.11075235,  1.46987273,  2.47463989]), 'split1_test_score': array([ 0.86757991,  0.86872146,  0.87100457,  0.88013699,  0.85045662,\n",
      "        0.86757991,  0.86643836,  0.86643836,  0.86986301,  0.87785388,\n",
      "        0.87442922,  0.87100457,  0.87328767,  0.87785388,  0.87214612])}\n"
     ]
    }
   ],
   "source": [
    "print grid.get_params()\n",
    "print grid.best_index_\n",
    "print grid.cv_results_['params'][grid.best_index_]\n",
    "print grid.best_params_\n",
    "print grid.best_score_\n",
    "print grid.best_estimator_\n",
    "print grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classify__hidden_layer_sizes': [600], 'classify__alpha': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "print grid.best_params_\n",
    "X_mal_SVD = SVD.transform(X_mat)\n",
    "X_test_SVD = SVD.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy of MLP model on training data:', 0.9115359688917692)\n"
     ]
    }
   ],
   "source": [
    "#Optimized MLP Classification \n",
    "mlp_final_model= MLPClassifier(alpha=1e-06, hidden_layer_sizes=(1000,))\n",
    "mlp_final_model.fit(X_mat, t_train)\n",
    "mlp_super_preds = mlp_final_model.predict(X_mat)\n",
    "B = t_train == mlp_super_preds \n",
    "\n",
    "acc = 1.*sum(B)/len(B)\n",
    "print(\"Accuracy of MLP model on training data:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91153596889176924"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_final_model.n_iter_\n",
    "accuracy_score(t_train,mlp_super_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3724, 1000)\n",
      "(3724,)\n",
      "done making predictions\n"
     ]
    }
   ],
   "source": [
    "print X_test_SVD.shape\n",
    "mlp_final_preds = mlp_final_model.predict(X_test)\n",
    "print mlp_final_preds.shape\n",
    "print(\"done making predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing predictions...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "outputfile = \"P2_MLP_WMB.csv\"\n",
    "print(\"writing predictions...\")\n",
    "util.write_predictions(mlp_final_preds, test_ids, outputfile)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
